##API CALL ON GEMINI-2.0-flash

import json
import requests
from helper_functions import read_entire_file, clear_file, prompt_generator, append_from_file, write_to_file, append_to_file, write_text_to_file
from acl_analyzer import compare_acl

def gemini_api_call(session_response, gt_acl_file, llm_abac_policy_file, policy_description_file):
    #Parameters
        #session_response: the file where you want to save the llm generated abac rules to
        # gt_acl_file: the acl file to feed to the LLM
        # llm_abac_policy_file: file with all user and resource information
        # attribute_despolicy_description_fileription_file the description of the attributes listed above.


    

    key_file ="llm-research/keys/geminiKey.txt"
    
    print("\nCalling gemini API...\n")

    try:
        gemini_key = read_entire_file(key_file)

    except FileNotFoundError as e:
        print(f"Error reading file: {e}")
        return
    except Exception as e:
        print(f"Unexpected read error: {e}")
        return
    
 # generated file #: declare the location on the complete request being made
    # this file should contain everyhting we are feeding the LLM to make the rules.
    complete_request_file = "llm-research/complete-prompt.txt"
    #generate the prompt, calls a helper function to combine all the text files into one.
    prompt_generator(gt_acl_file, llm_abac_policy_file, policy_description_file, complete_request_file)
    
    #pass prompt request from a .txt file to a string obj (required for the request)
    complete_request = read_entire_file("llm-research/complete-prompt.txt")


    # send to Gemini 
    url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent"
    headers = {"Content-Type": "application/json", "X-goog-api-key": gemini_key}
    data = {
        "contents": [
            {
                "parts": [
                    {"text": complete_request}
                ]
            }
        ]
    }

    try:
        resp = requests.post(url, headers=headers, json=data)
        resp.raise_for_status()
    except requests.exceptions.Timeout:
        print("HTTP error: request timed out")
        return
    except requests.exceptions.RequestException as e:
        print(f"HTTP error: {e}")
        return

    try:
        payload = resp.json()
    except json.JSONDecodeError:
        print("Response was not valid JSON.")
        return


    text = (
        payload.get("candidates", [{}])[0]
            .get("content", {})
            .get("parts", [{}])[0]
            .get("text", "")
    )

    #output the abac rules to a file for testing
    with open(session_response, "w", encoding="utf-8") as of:
        of.write(text)



    #for recurrsion
    #take arguements:
    #return a boolean if the ACLs match or we hit the TTL






    counter = 1
    #declare and clear file to resend the recursive prompt
    complete_request_file = "llm-research/session/recursive-session-prompt.txt"
    clear_file(complete_request_file)

    # write the llm respsonse to cache:  this should be a cache of only abac rules. 
        # unless the llm responds with anyhting else.
    string_obj= (f"you have generated this response on iteration # {counter}\n")
    append_to_file("llm-research/session/cache/session-llm-response.cache", string_obj)
    append_from_file("llm-research/session/cache/session-llm-response.cache", "llm-research/session/session-llm-response.txt")

    #write the acl list generated by the llm rules to cache
    string_obj =(f"We generated the access control list with your rules. Here is the access control list for iteration {counter}:\n")
    append_to_file("llm-research/session/cache/session-ACL.cache", string_obj)
    append_from_file("llm-research/session/cache/session-ACL.cache", "llm-research/session/session-ACL.txt")

    # write the comparison info to cache
    string_obj=(f"here is a comparison of the ground truth ACL and the ACL that was generated with your rules for iteration {counter} :\n")
    append_to_file("llm-research/session/cache/session-comparison.cache", string_obj)
    append_from_file("llm-research/session/cache/session-comparison.cache", "llm-research/session/session-comparison.txt")

    # let the llm know what iteration we are in
    string_obj= (f"\niteration # {counter} is the most current.\n")
    append_to_file(complete_request_file, string_obj)

    # add the LLM ABAC Responses to the new request
    append_from_file(complete_request_file, "llm-research/session/cache/session-llm-response.cache")

    # add the LLM ACL lists into the new request
    append_from_file(complete_request_file, "llm-research/session/cache/session-ACL.cache")

    # add the comparisons to the new request
    append_from_file(complete_request_file, "llm-research/session/cache/session-comparison.cache")

    # add instructions to ask for only abac rules
    string_obj = (f"Reply again with an improvement of the rules you made, reply ONLY with the new set of rules and NOTHING ELSE!\nHere is the original prompt, make sure to stick to it once again.\n")
    append_to_file(complete_request_file, string_obj)

    # add the original prompt
    append_from_file(complete_request_file, "llm-research/complete-prompt.txt")










 
# def itter_gemini_call(is_match, ttl,abac_rules_generated, acl_file, complete_request_file ):
#     # is_match is false if the ACLs are not a 100% match
#     # ttl is time to live for how many itterations we want to allow
#     # abac_rules_generated is the file to save the LLM response of ABAC rules
#     # acl_file is the Original ACL file, generated by our set of rules
#     # complete_request_file complete request file being sent to the LLM
#         #in this funciton it will be the re-requested file
    
#     print("Recalling gemini API")

#     counter = 1
   
#     #TODO: add the new request to the complete request
    
#     append_from_file(complete_request_file, "llm-research/complete-prompt.txt")
#     append_from_file(complete_request_file, "llm-research/re-request.txt")


#     while ( counter < ttl):

#     #TODO: add text to segment the info being sent 
#         if is_match == True:
#             print("100% match has been found")
#             return
        
#         else:
#             print(f"Re-calling gemini API \n Itteration number: {counter}\n")

#             write_to_file(complete_request_file, "THESE ARE THE RULES GENERATED BY YOU (THE LLM)")
#             #TODO: add the rules that the LLM generated 
#             append_from_file(complete_request_file, abac_rules_generated)
            
#             write_to_file(complete_request_file, "THIS IS THE ACL GENERATED BY YOUR ABAC RULES")
#             #TODO add the ACL generated by the rules
#             append_from_file(complete_request_file, "llm-research/llm-generated-data/gemini/gemini-healthcare/gemini-ACL.txt")
            
#             write_to_file(complete_request_file, "THESE ARE SOME NUMBERS GENERATED BY THE ACL COMPARISON")

#             #TODO: add the analytics that the comparison of the ACLs generated
#             temp_string, is_match = compare_acl(acl_file, "llm-research/llm-generated-data/gemini/gemini-healthcare/gemini-ACL.txt")
#             write_to_file(complete_request_file, temp_string)
#            #TODO: request the prompt again

#             clear_file(abac_rules_generated)

#             key_file ="llm-research/keys/geminiKey.txt"
            
#             print("\nCalling gemini API...\n")

#             try:
#                 gemini_key = read_entire_file(key_file)

#             except FileNotFoundError as e:
#                 print(f"Error reading file: {e}")
#                 return
#             except Exception as e:
#                 print(f"Unexpected read error: {e}")
#                 return
            
#         #declare the location on the complete request being made            
            
#             #pass prompt request from a .txt file to a string obj (required for the request)
#             complete_request = read_entire_file(complete_request_file)
            
#             # send to Gemini 
#             url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent"
#             headers = {"Content-Type": "application/json", "X-goog-api-key": gemini_key}
#             data = {
#                 "contents": [
#                     {
#                         "parts": [
#                             {"text": complete_request}
#                         ]
#                     }
#                 ]
#             }

#             try:
#                 resp = requests.post(url, headers=headers, json=data)
#                 resp.raise_for_status()
#             except requests.exceptions.Timeout:
#                 print("HTTP error: request timed out")
#                 return
#             except requests.exceptions.RequestException as e:
#                 print(f"HTTP error: {e}")
#                 return

#             try:
#                 payload = resp.json()
#             except json.JSONDecodeError:
#                 print("Response was not valid JSON.")
#                 return


#             text = (
#                 payload.get("candidates", [{}])[0]
#                     .get("content", {})
#                     .get("parts", [{}])[0]
#                     .get("text", "")
#             )

#             #output the abac rules to a file for testing
#             with open(abac_rules_generated, "w", encoding="utf-8") as of:
#                 of.write(text)


#         counter +=1

if __name__ == "__main__":
    gemini_api_call()
    print ("api call finalized")


##CURRENT ISSUE:M ACL NOT BEING GENERATED BY LLM REQUEST

#below is the orignal call incase i mess it up while making the recursive one

# def gemini_api_call(session_response, gt_acl_file, llm_abac_policy_file, policy_description_file):
#     #Parameters
#         #session_response: the file where you want to save the llm generated abac rules to
#         # gt_acl_file: the acl file to feed to the LLM
#         # llm_abac_policy_file: file with all user and resource information
#         # attribute_despolicy_description_fileription_file the description of the attributes listed above.


#     #clear the file to write over
#     clear_file(session_response)

#     key_file ="llm-research/keys/geminiKey.txt"
    
#     print("\nCalling gemini API...\n")

#     try:
#         gemini_key = read_entire_file(key_file)

#     except FileNotFoundError as e:
#         print(f"Error reading file: {e}")
#         return
#     except Exception as e:
#         print(f"Unexpected read error: {e}")
#         return
    
#  # generated file #: declare the location on the complete request being made
#     # this file should contain everyhting we are feeding the LLM to make the rules.
#     complete_request_file = "llm-research/complete-prompt.txt"
#     #generate the prompt, calls a helper function to combine all the text files into one.
#     prompt_generator(gt_acl_file, llm_abac_policy_file, policy_description_file, complete_request_file)
    
#     #pass prompt request from a .txt file to a string obj (required for the request)
#     complete_request = read_entire_file("llm-research/complete-prompt.txt")

#     # send to Gemini 
#     url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent"
#     headers = {"Content-Type": "application/json", "X-goog-api-key": gemini_key}
#     data = {
#         "contents": [
#             {
#                 "parts": [
#                     {"text": complete_request}
#                 ]
#             }
#         ]
#     }

#     try:
#         resp = requests.post(url, headers=headers, json=data)
#         resp.raise_for_status()
#     except requests.exceptions.Timeout:
#         print("HTTP error: request timed out")
#         return
#     except requests.exceptions.RequestException as e:
#         print(f"HTTP error: {e}")
#         return

#     try:
#         payload = resp.json()
#     except json.JSONDecodeError:
#         print("Response was not valid JSON.")
#         return


#     text = (
#         payload.get("candidates", [{}])[0]
#             .get("content", {})
#             .get("parts", [{}])[0]
#             .get("text", "")
#     )

#     #output the abac rules to a file for testing
#     with open(session_response, "w", encoding="utf-8") as of:
#         of.write(text)





